Thought process doc/reminders:
 
we dont give a pathing scaffolding, nor do we mask invalid actions, so logits much be close
update: had to use maybe too many masks to barely work (feels like "might as well write a bunch of if statements"), better approach needed

update2: maybe an even more extensive/adaptive mask and rewarding only winning and nothing else would still produce something interesting 
(ie, focusing on strategy instead of picking out "valid" actions), intermediate rewards depending on how many pieces player has could help with the 'sparsity'
also, could maybe train against (or to imitate) an improved SimpleAI and punish for losing against it

update3: made game more strategic instead of a stalemate slog by adding an economy, masking being overhauled is planned next as even now most actions are invalid. 
Also, fixed a massive oversight of the next_state the network sees being from the POV of the opponent.
I'm using the proportion of how many games get resolved within a 1000 timesteps as a preliminary evaluation heurisic at convergence and the best performance so far is around 30%. Getting better!

update4: 40-50%
                                                                                             
19/12

Found the time and motivation to come back to this.
Okay, turns out increasing lambda is all you need. Which is obvious since we have a strategy game here.

20/12

-I've decided to use reward per timestep as an evaluation metric as it is higher if the model gets on with it quickly. I suspect there are soft-lock states that mess with training. 
-Still haven't hardcoded complete masking.
-I think shorter episodes are preferred as the states become less correlated and the end game states aren't that different from beginning ones anyway. There're states where the
AI is stuck though and does many invalid actions, and maybe it's good for the value head to discourage them... No clue.
-The soft-lock states may be that the AI just spent all their money and there's no path to the enemy due to terrain. (Say, it built only soldiers but there's a body of water that needs to be crossed).
Maybe changing the game logic to include cash-outs would work but that would mess with debt mechanic and unit costs. And it seems I've tuned those quite well two months ago
because touching them just seems to make the game dumber/harder to resolve, or at least seems so from watching the visualization during training.

-24/12 EDIT: Still did it to see where it's struggling 23/12 EDIT: No longer needed. Invalid actions are masked out. TO DO: Should include terrain in the terminal visualization. Should track invalid actions.
-19/01 EDIT: Done! TO DO: Use the number of turns to beat (if it is beaten) the hard-coded SimpleAI for testing eval as we do not train on it.
-Maybe passing through linear layer(s) before the softmax would be any good? Probably not though since the "classes" are still grid-like.
-The way how output grid and action type heads are related to each and the whole decision-making sequence in the architecture is most likely very flawed.

21/12
-While refractoring the mess and overhauling masking and training I found a small decision, that didn't think much about and forgot. Basically made actor loss negligible
after two hundred steps or so into an episode (and I was wondering why it wasn't training! 80-96% of the gradient steps only considered what the value head was doing...)
I would guess it has been actually incapable so far of learning longer games, where, say, the players spawned far from each other. Probably most of what caused the softlocks.
-EDIT 09/05: TBH I think this doesn't matter too much, maybe if I change the training regime TO DO: The next state that we use for td_target is not really the next state as it does not account for the action of the opponent. Not sure how to do this without experience replay.
-The biggest issue right now is that I have implemented action rules/validity in four places: the SimpleAI, GUI, the environment and in training (for masking). Should just be in game logic.
-This project is way too fun but I'm not sure how much time to dedicate to it. I'd like to try different architectures, expand the game etc. Make visualizations of logits in board states for interpretability.

22/12
-The action type really depends on sources and targets. I'm thinking of having action type also decided by a grid where we put all values through a sigmoid.

23/12
-No longer a separate action type head, different targets have different action types assigned to them. Finished masking.

24/12
-Even with masks the training results are seemingly unimpressive. Could just be self-play and equal opponents though. Or the game
design is bad and it takes too long. Next step is to implement an eval against a hard-coded AI.
-I'm going to make the game map slightly smaller, to have it conclude faster. Also, I'm considering changing the rules such that you
can move multiple pieces per turn. Would simplify the network a lot for the very least.
-TO DO: I'm thinking of trying a new training approach where the model plays against an older version of itself rather than itself. Should stabilize.
-After visualization, I see my guess about soft-locks was correct. The players can't reach each other due to terrain and running out of money.
-Eligibility traces and advantage do not interact as they should.
-Gotta fix soft locks. An idea is pay gold to turn an empty tile into land/water (ships can build canals and soldiers bridges). Or
the unit could sacrifice itself in order to change tile, to account for debt states.
-Okay, but there is no good reason why it should fail to learn to get over the river. And to make a plan. And not just spam as much units
as soon as possible. 
-I suspect the current residual connection just makes the network not use most layers. Will adjust.

26/12
-Probably just stacking convolutions like that does not work for decision-making. I'm gonna make a new architecture with proper
residual blocks and slapping on a transformer encoder at the end for global associations. Maybe it'll cross rivers then...
-If not, MCTS it is.

19/01
-Coming back!! 
-I really need to finish this
-Okay, doing an eval. The network playing against itself with greedy sampling just gets stuck in a loop doing nothing. It seems like it hasn't trained to play well at all and would just end up doing things
by randomly following things where immediate reward is expected. I could sample from a softmax in eval as well tho, but that would be cheating.Best action should be best action. Let's see how it fares against SimpleAI.
-Ran an eval against SimpleAI, the SimpleAI wins 76% of the games, 18% being draws, and the CNN winning 6%. RIP
-Gonna try a resnet, with some linear layers inbetween
20/01
-TO DO: Eligibility traces are still messed up, need to look up code online and ADAM interactions
-All the ResNets I've tested seem to be doing worse than the more vanilla-ish net so far.
-I've increased game complexity slightly faster than model capacity throughout. Game design kinda sucks, but whatever adjustments I can think of would make it too easy to learn.
-The problem I'd say is the nets go into a minima where they just spend all their money instantly, and low time-preference SimpleAI crushes them. Plus latter has proper pathfinding (I'm not sure
whether the nets are learning the latter, from the visualizations it seems like there's only pointless movement when there's nowhere to go).
-I think I'm going to switch up the training regime now. Without a value head (that needs flattening and a linear layer), I could go with curriculum learning, increasing the map size every few epochs.
-Managed to train a resnet with 11% win, 33% draw. Improving. Going to find the best kind of block out of the few I'm considering and then try a new training approach. Or maybe even attention?

09/05
-It has been weighing on me that I didn't finish this. Going to try to get something one last time.
-Implemented curriculum learning, the architecture now can do any map size.
-Added embeddings
-Embeddings have improved performance significantly, excluding draws the model has an overall winrate of 30-40% vs. SimpleAI

