Thought process doc/reminders:
 
we dont give a pathing scaffolding, nor do we mask invalid actions, so logits much be close
update: had to use maybe too many masks to barely work (feels like "might as well write a bunch of if statements"), better approach needed

update2: maybe an even more extensive/adaptive mask and rewarding only winning and nothing else would still produce something interesting 
(ie, focusing on strategy instead of picking out "valid" actions), intermediate rewards depending on how many pieces player has could help with the 'sparsity'
also, could maybe train against (or to imitate) an improved SimpleAI and punish for losing against it

update3: made game more strategic instead of a stalemate slog by adding an economy, masking being overhauled is planned next as even now most actions are invalid. 
Also, fixed a massive oversight of the next_state the network sees being from the POV of the opponent.
I'm using the proportion of how many games get resolved within a 1000 timesteps as a preliminary evaluation heurisic at convergence and the best performance so far is around 30%. Getting better!

update4: 40-50%
                                                                                             
19/12

Found the time and motivation to come back to this.
Okay, turns out increasing lambda is all you need. Which is obvious since we have a strategy game here.

20/12

-I've decided to use reward per timestep as an evaluation metric as it is higher if the model gets on with it quickly. I suspect there are soft-lock states that mess with training. 
-Still haven't hardcoded complete masking.
-I think shorter episodes are preferred as the states become less correlated and the end game states aren't that different from beginning ones anyway. There're states where the
AI is stuck though and does many invalid actions, and maybe it's good for the value head to discourage them... No clue.
-The soft-lock states may be that the AI just spent all their money and there's no path to the enemy due to terrain. (Say, it built only soldiers but there's a body of water that needs to be crossed).
Maybe changing the game logic to include cash-outs would work but that would mess with debt mechanic and unit costs. And it seems I've tuned those quite well two months ago
because touching them just seems to make the game dumber/harder to resolve, or at least seems so from watching the visualization during training.

-TO DO: Should include terrain in the terminal visualization. Should track invalid actions.
-TO DO: Use the number of turns to beat (if it is beaten) the hard-coded SimpleAI for testing eval as we do not train on it.
-Maybe passing through linear layer(s) before the softmax would be any good? Probably not though since the "classes" are still grid-like.
-The way how output grid and action type heads are related to each and the whole decision-making sequence in the architecture is most likely very flawed.

21/12
-While refractoring the mess and overhauling masking and training I found a small decision, that didn't think much about and forgot. Basically made actor loss negligible
after two hundred steps or so into an episode (and I was wondering why it wasn't training! 80-96% of the gradient steps only considered what the value head was doing...)
I would guess it has been actually incapable so far of learning longer games, where, say, the players spawned far from each other. Probably most of what caused the softlocks.
-TO DO: The next state that we use for td_target is not really the next state as it does not account for the action of the opponent. Not sure how to do this without experience replay.
-The biggest issue right now is that I have implemented action rules/validity in four places: the SimpleAI, GUI, the environment and in training (for masking). Should just be in game logic.
-This project is way too fun but I'm not sure how much time to dedicate to it. I'd like to try different architectures, expand the game etc. Make visualizations of logits in board states for interpretability.

22/12
I think once I learn more stuff I could try weirder architectures etc. Make the game more complicated and use a GNN or something.
-The action type really depends on sources and targets. I'm thinking of having action type also decided by a grid where we put all values through a sigmoid.